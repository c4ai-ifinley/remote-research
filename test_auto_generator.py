#!/usr/bin/env python3
"""
Test script for LLM-powered auto-generation of test cases
"""

from test_case_generator import auto_generate_test_cases, TestCaseGenerator
from llm_test_generator import LLMTestCaseGenerator, ToolInfo, llm_generate_test_cases
from color_utils import (
    test_colors,
    success_print,
    system_print,
    debug_print,
    dspy_print,
    warning_print,
)
import json
import os


def test_llm_generation():
    """Test the LLM-powered auto-generation functionality"""
    test_colors()

    system_print("Testing LLM-powered auto-generation of test cases...")

    # Create a mock chatbot with tools
    class MockChatbot:
        def __init__(self):
            self.available_tools = [
                {
                    "name": "search_papers",
                    "description": "Searches academic papers on arXiv based on topic and returns paper IDs",
                    "input_schema": {
                        "properties": {
                            "topic": {
                                "type": "string",
                                "description": "Research topic",
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "Maximum results",
                            },
                        }
                    },
                },
                {
                    "name": "read_file",
                    "description": "Reads and returns the contents of a specified file",
                    "input_schema": {
                        "properties": {
                            "path": {
                                "type": "string",
                                "description": "File path to read",
                            }
                        }
                    },
                },
                {
                    "name": "custom_ai_tool",
                    "description": "A custom AI tool that processes data using machine learning",
                    "input_schema": {
                        "properties": {
                            "data": {
                                "type": "string",
                                "description": "Input data to process",
                            },
                            "model": {
                                "type": "string",
                                "description": "ML model to use",
                            },
                        }
                    },
                },
            ]

    mock_chatbot = MockChatbot()
    test_config_path = "test_llm_generated_demo.json"

    # Clean up any existing test file
    if os.path.exists(test_config_path):
        os.remove(test_config_path)

    # Test LLM generation
    dspy_print("Testing LLM generation...")
    available_tools = [tool["name"] for tool in mock_chatbot.available_tools]

    success = llm_generate_test_cases(available_tools, mock_chatbot, test_config_path)

    if success:
        success_print("LLM auto-generation successful!")

        # Display what was generated
        with open(test_config_path, "r") as f:
            config = json.load(f)

        system_print("LLM-generated test cases:")
        for tool_name, test_cases in config["test_cases"].items():
            debug_print(f"  {tool_name}: {len(test_cases)} test case(s)")
            for test_case in test_cases:
                debug_print(
                    f"    - {test_case['test_name']}: {test_case['description']}"
                )
                debug_print(f"      Prompt: {test_case['prompt']}")
                debug_print(
                    f"      Generated by: {test_case.get('generated_by', 'unknown')}"
                )

        success_print(f"LLM configuration saved to: {test_config_path}")

        # Test validation
        generator = LLMTestCaseGenerator(test_config_path)
        if generator._validate_test_case(config["test_cases"][available_tools[0]][0]):
            success_print("LLM-generated test case validation passed!")
        else:
            warning_print("LLM-generated test case validation failed")

    else:
        warning_print("LLM generation not available, testing fallback...")

        # Test fallback generation
        if auto_generate_test_cases(available_tools, test_config_path):
            success_print("Rule-based fallback successful!")
        else:
            system_print("No test cases needed to be generated")


def test_individual_tool_generation():
    """Test generating test cases for individual tools"""
    system_print("\nTesting individual tool generation...")

    generator = LLMTestCaseGenerator("test_individual.json")

    # Test with a specific tool
    test_tool = ToolInfo(
        name="analyze_data",
        description="Analyzes datasets using statistical methods and returns insights",
        parameters={
            "dataset_path": {
                "type": "string",
                "description": "Path to the dataset file",
            },
            "analysis_type": {
                "type": "string",
                "description": "Type of analysis to perform",
            },
            "output_format": {"type": "string", "description": "Format for results"},
        },
        output_format="json_or_text",
        examples=[
            "Analyze sales data for trends and patterns",
            "Perform statistical analysis on user behavior data",
            "Generate insights from marketing campaign data",
        ],
    )

    test_cases = generator.generate_test_cases_for_tool(test_tool)

    if test_cases:
        success_print(f"Generated {len(test_cases)} test cases for {test_tool.name}")
        for i, case in enumerate(test_cases, 1):
            debug_print(f"  {i}. {case['test_name']}")
            debug_print(f"     Description: {case['description']}")
            debug_print(f"     Prompt: {case['prompt']}")
            debug_print(f"     Critical: {case['critical']}")
    else:
        warning_print("No test cases generated")


def compare_generation_methods():
    """Compare LLM vs rule-based generation"""
    system_print("\nComparing LLM vs Rule-based generation...")

    # Mock tools for comparison
    test_tools = ["search_papers", "custom_unknown_tool"]

    # Generate with LLM
    llm_generator = LLMTestCaseGenerator("test_llm_compare.json")

    # Generate with rule-based
    rule_generator = TestCaseGenerator("test_rule_compare.json")

    for tool_name in test_tools:
        debug_print(f"\nTesting tool: {tool_name}")

        # LLM generation (if available)
        if llm_generator.generator:
            mock_tool = ToolInfo(
                name=tool_name,
                description=f"Tool for {tool_name} operations",
                parameters={"param1": {"type": "string"}},
                output_format="text",
                examples=[f"Use {tool_name} for testing"],
            )
            llm_cases = llm_generator.generate_test_cases_for_tool(mock_tool)
            debug_print(f"  LLM generated: {len(llm_cases)} test cases")
        else:
            debug_print("  LLM not available")

        # Rule-based generation
        rule_cases = rule_generator._generate_test_cases_for_tool(tool_name)
        debug_print(f"  Rule-based generated: {len(rule_cases)} test cases")


if __name__ == "__main__":
    test_llm_generation()
    test_individual_tool_generation()
    compare_generation_methods()

    success_print("\nAll tests completed!")
#!/usr/bin/env python3
"""
Test script for auto-generation of test cases
"""

from test_case_generator import auto_generate_test_cases, TestCaseGenerator
from color_utils import test_colors, success_print, system_print, debug_print
import json
import os


def test_auto_generation():
    """Test the auto-generation functionality"""
    test_colors()

    system_print("Testing auto-generation of test cases...")

    # Test with a variety of tools
    test_tools = [
        "read_file",
        "write_file",
        "list_directory",
        "search_papers",
        "extract_info",
        "fetch",
        "unknown_tool_xyz",  # Should use generic template
        "custom_database_tool",  # Should use generic template
    ]

    test_config_path = "test_generated_demo.json"

    # Clean up any existing test file
    if os.path.exists(test_config_path):
        os.remove(test_config_path)

    # Generate test cases
    success = auto_generate_test_cases(test_tools, test_config_path)

    if success:
        success_print("Auto-generation successful!")

        # Display what was generated
        with open(test_config_path, "r") as f:
            config = json.load(f)

        system_print("Generated test cases:")
        for tool_name, test_cases in config["test_cases"].items():
            debug_print(f"  {tool_name}: {len(test_cases)} test case(s)")
            for test_case in test_cases:
                debug_print(
                    f"    - {test_case['test_name']}: {test_case['description']}"
                )

        success_print(f"Configuration saved to: {test_config_path}")

        # Test validation
        generator = TestCaseGenerator(test_config_path)
        if generator.validate_config_schema():
            success_print("Schema validation passed!")
        else:
            system_print("Schema validation failed")

    else:
        system_print("No test cases needed to be generated")


if __name__ == "__main__":
    test_auto_generation()
